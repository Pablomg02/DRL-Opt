{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andador\n",
    "En este archivo se lleva a cabo el entrenamiento de un modelo más complejo, logrando que un robot con dos piernas sea capaz de caminar.\n",
    "Este mismo archivo ha sido utilizado para entrenar el modelo con diferentes hiperparamtros. Más adelante se pueden modificar los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente dirección se guardan (o cargan) los modelos a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/bipedal_good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso, solo se modifican los siguientes parámetros. `num_cpu` y `test_num_cpu` se puede modificar dado que se ha creado un entorno vectorizado, una herramienta muy útil para aprovechar el multiprocesamiento de la CPU o de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu = 6  # Numero de nucleos a utilizar para el entrenamiento\n",
    "test_num_cpu = 1 # Numero de nucleos a utilizar para la evaluacion. Dejar en 1 para evitar errores\n",
    "total_timesteps = 2000000 \n",
    "env_id = \"BipedalWalker-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A continuación, se pueden modificar tanto los hiperparámetros como la arquitectura de la red***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HIPERPARAMETROS\n",
    "n_steps = 2048\n",
    "batch_size = 64\n",
    "gae_lambda = 0.95\n",
    "gamma = 0.999\n",
    "n_epochs = 10\n",
    "ent_coef = 0.0\n",
    "learning_rate = 3e-4\n",
    "clip_range = 0.18\n",
    "\n",
    "#### ARQUITECTURA DE LA RED\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[256, 256], vf=[256, 256])]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean los entornos, así como un objeto para hacer un seguimiento del entrenamiento y poder guardar el mejor modelo obtenido, que no necesariamente es al final del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = make_vec_env(env_id, n_envs=num_cpu)\n",
    "\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=path,\n",
    "                                log_path=path, eval_freq=70000 // num_cpu,\n",
    "                                n_eval_episodes=7, deterministic=True,\n",
    "                                render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación del modelo. Además, se definen los ajustes adicionales de la arquitectura e hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", vec_env, verbose=0, tensorboard_log=path,\n",
    "            gamma=gamma, n_steps=n_steps, ent_coef=ent_coef, batch_size=batch_size,\n",
    "            gae_lambda=gae_lambda, learning_rate=learning_rate, clip_range=clip_range,\n",
    "            n_epochs=n_epochs,\n",
    "            policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entrenamiento se realiza a continuación. Los outputs se encuentran colapsados para evitar ocupar demasiado espacio, pero se pueden descolapsar para ver cuál ha sido el procedimiento.\n",
    "\n",
    "**ADVERTENCIA: No es necesario entrenar de nuevo el modelo, ya que se encuentra guardado en la carpeta `models`. Se puede saltar al siguiente bloque de código**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=total_timesteps, progress_bar=True, callback=eval_callback, tb_log_name=\"TB_LOG\")\n",
    "\n",
    "del model # Se borra para que luego se pueda cargar el modelo sin problemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder probar el modelo, aunque se podria usar el entorno `vec_env`, vamos a crear un entorno individual para evitar errores (a veces Stable Baselines 3 y PyTorch dan problemas de compatibilidad, y así se ahorran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el modelo de la carpeta `models`, que corresponde al entrenado anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(f\"{path}/best_model\", env=eval_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `evaluate_policy` permite examinar el modelo en el escenario, proporcionando una media de recompensas obtenidas y su desviación típica. Dado el rendimiento del algoritmo PPO y la simpleza del caso, consigue una precisión perfecta en todas las circunstancias (o, al menos, en casi todas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">mean_reward:283.09 +/- 1.67\n",
       "</pre>\n"
      ],
      "text/plain": [
       "mean_reward:283.09 +/- 1.67\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=40)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se puede visualizar una secuenciad e iteraciones del modelo. Como se visualizó en la evaluación del modelo, la precisión será perfecta.\n",
    "\n",
    "***Nota: En ocasiones, ipynb da problemas para visualizar la ejecución. En ese caso, utilizar el codigo en la carpeta `otros`.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = eval_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = eval_env.step(action)\n",
    "    eval_env.render(\"human\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
